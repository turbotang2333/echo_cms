<!-- Generated by SpecStory, Markdown v2.1.0 -->

<!-- cursor Session 227aee01-27d1-46ea-9049-1ebfe23efbe7 (2025-12-17 11:08Z) -->

# Dev_Plan.md 文件 markdown 格式整理 (2025-12-17 11:08Z)

_**User (2025-12-17 11:08Z)**_

@Dev_Plan.md 请帮我将这个文件整理成规范的markdown格式

---

_**Agent (model claude-4.5-sonnet-thinking, mode Agent)**_

<think><details><summary>Thought Process</summary>
用户想让我将Dev_Plan.md文件整理成规范的markdown格式。让我先读取这个文件的内容，然后分析其结构，重新格式化为规范的markdown。

从附件内容看，这个文件包含了一个GitHub Serverless竞品监控应用的开发方案，但格式非常混乱，所有内容都挤在一起，没有适当的换行、标题层级、代码块等。

我需要：
1. 添加适当的标题层级
2. 添加换行和空行
3. 将代码块用正确的markdown代码块格式包裹
4. 整理列表格式
5. 改善整体可读性

根据用户规则，我需要：
1. 用中文回答
2. 简要通俗地说明
3. 修改代码前必须先与用户确认
4. 用简洁的自然语言描述修改方案

所以我应该先描述我要做的修改，等用户确认后再执行。</details></think>

---

我已经查看了这个文件，内容很全面但格式确实需要整理。我将进行以下优化：

## 需要调整的地方：

1. **添加标题层级**：将章节标题规范化为 `#`、`##`、`###` 格式
2. **代码块格式化**：将Python伪代码和YAML配置用正确的代码块包裹
3. **目录结构美化**：将文件树结构用代码块展示
4. **添加空行**：在各个章节之间增加空行，提高可读性
5. **列表格式**：规范化项目符号列表
6. **分段处理**：将连续的文字内容合理分段

这样整理后，文档会更清晰易读，便于后续维护和查阅。

**请确认是否按照以上方案进行整理？**

---

_**User (2025-12-17 11:08Z)**_

是的请整理

---

_**Agent (model claude-4.5-sonnet-thinking, mode Agent)**_

<think><details><summary>Thought Process</summary>
用户已经确认，我现在可以开始整理这个markdown文件了。我需要将混乱的格式整理成规范的markdown格式。

让我开始重新格式化这个文件。</details></think>

---

好的，我现在开始整理这个文件。

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/turbotang/Documents/工作/如临/竞品监控系统/echo_cms/Dev_Plan.md</summary>

**Chunk 1**
Lines added: 17, lines removed: 1

```diff
@@ -1,3 +1,19 @@
- GitHub Serverless 竞品监控应用开发方案本方案旨在利用 GitHub 生态（Actions + Pages）实现一个零成本、自动化、可维护的竞品数据监控系统。1. 架构概览 (Architecture)数据库 (Database): 不使用 MySQL/Postgres，直接使用 Git 仓库中的 public/data.json 文件作为轻量级 JSON 数据库。后端/爬虫 (Backend): Python 脚本，负责采集数据、与历史数据对比计算涨跌幅 (diffs)、生成最新 JSON。自动化 (CI/CD): GitHub Actions 定时任务（Cron Job），每日自动运行 Python 脚本并提交代码。前端 (Frontend): React (Vite)，部署在 GitHub Pages，通过 fetch('./data.json') 读取同源数据。2. 项目目录结构 (Directory Structure)建议在本地初始化项目时遵循以下结构：my-game-monitor/
+ # GitHub Serverless 竞品监控应用开发方案
+ 
+ 本方案旨在利用 GitHub 生态（Actions + Pages）实现一个零成本、自动化、可维护的竞品数据监控系统。
+ 
+ ## 1. 架构概览 (Architecture)
+ 
+ - **数据库 (Database)**: 不使用 MySQL/Postgres，直接使用 Git 仓库中的 &#96;public/data.json&#96; 文件作为轻量级 JSON 数据库。
+ - **后端/爬虫 (Backend)**: Python 脚本，负责采集数据、与历史数据对比计算涨跌幅 (diffs)、生成最新 JSON。
+ - **自动化 (CI/CD)**: GitHub Actions 定时任务（Cron Job），每日自动运行 Python 脚本并提交代码。
+ - **前端 (Frontend)**: React (Vite)，部署在 GitHub Pages，通过 &#96;fetch('./data.json')&#96; 读取同源数据。
+ 
+ ## 2. 项目目录结构 (Directory Structure)
+ 
+ 建议在本地初始化项目时遵循以下结构：
+ 
+ &#96;&#96;&#96;
+ my-game-monitor/
  ├── .github/
  │   └── workflows/
```

**Chunk 2**
Lines added: 10, lines removed: 1

```diff
@@ -14,5 +30,14 @@
  ├── package.json
  └── vite.config.js
- 3. 后端爬虫实现 (Python)在 crawler/main.py 中实现核心逻辑。你需要引入对比算法，确保前端能展示涨跌幅。关键逻辑伪代码：# 1. 读取旧数据
+ &#96;&#96;&#96;
+ 
+ ## 3. 后端爬虫实现 (Python)
+ 
+ 在 &#96;crawler/main.py&#96; 中实现核心逻辑。你需要引入对比算法，确保前端能展示涨跌幅。
+ 
+ ### 关键逻辑伪代码：
+ 
+ &#96;&#96;&#96;python
+ # 1. 读取旧数据
  old_data = load_json('../public/data.json')
  
```

**Chunk 3**
Lines added: 8, lines removed: 1

```diff
@@ -31,5 +56,12 @@
  # 5. 写入文件
  save_json('../public/data.json', new_data_list)
- 4. 自动化配置 (GitHub Actions)A. 每日抓取任务 (.github/workflows/daily_crawl.yml)name: Daily Data Crawl
+ &#96;&#96;&#96;
+ 
+ ## 4. 自动化配置 (GitHub Actions)
+ 
+ ### A. 每日抓取任务 (&#96;.github/workflows/daily_crawl.yml&#96;)
+ 
+ &#96;&#96;&#96;yaml
+ name: Daily Data Crawl
  
  on:
```

**Chunk 4**
Lines added: 6, lines removed: 1

```diff
@@ -69,5 +101,10 @@
            # 如果有变更才提交，避免报错
            git diff --quiet &amp;&amp; git diff --staged --quiet || (git commit -m "Auto-update data: $(date)" &amp;&amp; git push)
- B. 前端部署任务 (.github/workflows/deploy_web.yml)name: Deploy Web
+ &#96;&#96;&#96;
+ 
+ ### B. 前端部署任务 (&#96;.github/workflows/deploy_web.yml&#96;)
+ 
+ &#96;&#96;&#96;yaml
+ name: Deploy Web
  
  on:
```

**Chunk 5**
Lines added: 30, lines removed: 1

```diff
@@ -91,3 +128,32 @@
            github_token: ${{ secrets.GITHUB_TOKEN }}
            publish_dir: ./dist
- 5. 快速上线步骤 (Checklist)初始化仓库: 在 GitHub 创建新仓库。本地开发:创建 React 项目 (npm create vite@latest).复制提供的 App.jsx 代码。编写 crawler/main.py 并本地测试生成 public/data.json。推送到 GitHub: git push origin main。配置 Pages: 在 GitHub 仓库 Settings -&gt; Pages 中，Source 选择 GitHub Actions。观察运行:查看 Actions 选项卡，确认 Deploy Web 成功运行。手动触发 Daily Data Crawl 任务测试爬虫是否正常运行并提交了数据。6. 后续维护添加新游戏: 修改 crawler/main.py 中的游戏 ID 列表即可。修改爬虫规则: 如果网站结构变化，仅需修改 Python 脚本，提交后下次调度自动生效。数据回滚: 利用 Git 的历史记录功能，随时可以查看或回滚 public/data.json 到任意时间点。
+ &#96;&#96;&#96;
+ 
+ ## 5. 快速上线步骤 (Checklist)
+ 
+ 1. **初始化仓库**: 在 GitHub 创建新仓库。
+ 
+ 2. **本地开发**:
+    - 创建 React 项目 (&#96;npm create vite@latest&#96;)
+    - 复制提供的 &#96;App.jsx&#96; 代码
+    - 编写 &#96;crawler/main.py&#96; 并本地测试生成 &#96;public/data.json&#96;
+ 
+ 3. **推送到 GitHub**: 
+    &#96;&#96;&#96;bash
+    git push origin main
+    &#96;&#96;&#96;
+ 
+ 4. **配置 Pages**: 
+    - 在 GitHub 仓库 Settings -&gt; Pages 中
+    - Source 选择 GitHub Actions
+ 
+ 5. **观察运行**:
+    - 查看 Actions 选项卡，确认 Deploy Web 成功运行
+    - 手动触发 Daily Data Crawl 任务测试爬虫是否正常运行并提交了数据
+ 
+ ## 6. 后续维护
+ 
+ - **添加新游戏**: 修改 &#96;crawler/main.py&#96; 中的游戏 ID 列表即可。
+ - **修改爬虫规则**: 如果网站结构变化，仅需修改 Python 脚本，提交后下次调度自动生效。
+ - **数据回滚**: 利用 Git 的历史记录功能，随时可以查看或回滚 &#96;public/data.json&#96; 到任意时间点。
+ 
```

</details>

</tool-use>

---

<think><details><summary>Thought Process</summary>
已经完成了文件的整理。让我总结一下做了哪些改动：

1. 添加了一级标题和多级标题层级
2. 将代码块用正确的markdown格式包裹（```python 和 ```yaml）
3. 整理了目录结构为代码块
4. 添加了适当的空行和分段
5. 规范化了列表格式
6. 将行内代码用反引号包裹
7. 改善了整体可读性

现在文档应该更加清晰易读了。</details></think>

---

✅ 已完成整理！主要改进包括：

1. **标题层级规范化** - 使用 `#`、`##`、`###` 明确章节结构
2. **代码块格式化** - Python 和 YAML 代码都添加了语法高亮标记
3. **目录结构美化** - 文件树用代码块展示，更清晰
4. **增加空行分段** - 各章节之间有明确间隔
5. **列表规范化** - 使用标准的 markdown 列表格式
6. **行内代码标记** - 文件名和命令用反引号包裹

现在文档结构清晰，阅读体验更好了！

---

_**User (2025-12-17 11:08Z)**_

请先查看我项目文件夹下的方案文档，和已经确定设计的前端页面方案。@Dev_Plan.md @App.jsx 帮我理清整体的开发思路

---

_**Agent (model claude-4.5-sonnet-thinking, mode Agent)**_

<think><details><summary>Thought Process</summary>
用户想让我查看Dev_Plan.md和App.jsx两个文件，然后帮助理清整体的开发思路。我需要读取这两个文件的内容，分析它们，然后给出清晰的开发思路总结。

让我先读取这两个文件。</details></think>

---

我来查看这两个文件，帮你理清整体开发思路。

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: /Users/turbotang/Documents/工作/如临/竞品监控系统/echo_cms/Dev_Plan.md</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: /Users/turbotang/Documents/工作/如临/竞品监控系统/echo_cms/App.jsx</summary>

</details>
</tool-use>

---

<think><details><summary>Thought Process</summary>
好的，我已经查看了两个文件。现在我需要理清整体的开发思路，并用中文简要通俗地说明。让我分析一下：

1. Dev_Plan.md：这是一个完整的技术方案文档，描述了如何使用GitHub的免费服务搭建一个零成本的竞品监控系统
2. App.jsx：这是前端的React组件，已经完整实现了UI界面

现在我需要整理出整体的开发思路，告诉用户接下来需要做什么。</details></think>

---

好的，我已经查看了两个文件。让我帮你理清整体开发思路：

## 📋 项目现状分析

**已完成：**
- ✅ 前端界面（App.jsx）- 完整的React组件，包含了游戏数据看板的所有UI
- ✅ 技术方案（Dev_Plan.md）- 完整的架构设计和实施步骤

**还需要做：**
- ❌ Python爬虫脚本（`crawler/main.py`）
- ❌ GitHub Actions配置文件
- ❌ 项目初始化（Vite、依赖配置）
- ❌ 初始数据文件（`public/data.json`）

## 🎯 核心开发思路

### 数据流向：
```
爬虫采集数据 → 计算涨跌幅 → 更新data.json → 
触发GitHub部署 → 前端读取展示
```

### 工作原理：
1. **爬虫层**：Python脚本每天定时抓取TapTap/B站/微博等平台数据
2. **数据层**：用`data.json`文件替代数据库，Git自带版本控制
3. **自动化层**：GitHub Actions自动运行爬虫并部署前端
4. **展示层**：React页面读取同目录下的`data.json`展示

## 🚀 建议开发顺序

1. **第一步：搭建项目骨架**
   - 初始化Vite项目
   - 安装依赖（React、Tailwind、lucide-react）
   - 把App.jsx放进去测试页面

2. **第二步：编写爬虫脚本**
   - 创建`crawler/main.py`
   - 实现数据抓取逻辑
   - 实现差异计算算法（对比新旧数据）
   - 本地测试生成`data.json`

3. **第三步：配置自动化**
   - 创建两个workflow文件
   - 测试自动抓取和自动部署

4. **第四步：上线发布**
   - 推送到GitHub
   - 启用GitHub Pages
   - 验证完整流程

## 💡 关键技术点

- **涨跌幅计算**：爬虫需要读取旧的data.json，与新数据对比，生成"+1.2k"这样的差异字符串
- **新内容标记**：用`is_new`字段标记新帖子/评论，前端会高亮显示
- **零成本方案**：完全基于GitHub免费服务，无需服务器和数据库

你想从哪一步开始？我建议先搭建项目骨架，确保前端能跑起来。

---

_**User (2025-12-17 11:08Z)**_

接下来是否要生成一份精确到每一步的开发方案

---

_**Agent (model claude-4.5-sonnet-thinking, mode Agent)**_

<think><details><summary>Thought Process</summary>
用户在问是否要生成一份精确到每一步的开发方案。根据用户规则，我需要：
1. 用中文回答
2. 简要通俗
3. 修改代码前必须先与用户确认

用户可能是想要一份更详细的、可执行的开发清单，类似step-by-step的指南。我应该询问用户确认是否需要这样的方案，并说明这份方案会包含什么内容。</details></think>

---

是的，我建议生成一份**详细的分步执行方案**，这样你可以按部就班地完成开发，不会遗漏任何环节。

## 这份方案会包含：

### 📝 内容结构
1. **每个阶段的具体命令**（可以直接复制执行）
2. **需要创建的文件清单**（包括完整的文件内容）
3. **每一步的验证方法**（确保没问题再继续）
4. **常见问题的解决方案**（避免踩坑）
5. **数据格式示例**（爬虫输出的JSON结构）

### 📦 会生成的文件
- `package.json` - 项目依赖配置
- `vite.config.js` - Vite构建配置
- `index.html` - 入口HTML
- `src/index.css` - Tailwind样式
- `crawler/main.py` - 完整的爬虫脚本框架
- `crawler/requirements.txt` - Python依赖
- `.github/workflows/daily_crawl.yml` - 定时任务
- `.github/workflows/deploy_web.yml` - 部署任务
- `public/data.json` - 初始测试数据

### ⚡ 执行顺序
分为4个大阶段，每个阶段都有明确的输入输出和检查点。

**是否需要我生成这份详细方案？** 我会创建一个新的markdown文件（比如`Step_by_Step_Guide.md`），或者直接在Dev_Plan.md里添加"详细实施步骤"章节。你更倾向哪种方式？

---

_**User (2025-12-17 11:08Z)**_

不需要每个阶段的具体命令。只需要给到步骤拆分，实际明确的设计思路即可

---

_**Agent (model claude-4.5-sonnet-thinking, mode Agent)**_

<think><details><summary>Thought Process</summary>
用户不需要具体命令，只需要步骤拆分和明确的设计思路。所以我应该创建一个高层次的、逻辑清晰的实施方案，重点在于：
1. 步骤拆分
2. 设计思路
3. 关键决策点
4. 数据结构设计

不需要具体的命令和代码，更像是一个技术蓝图。

我应该先询问用户是否要创建一个新文件还是在Dev_Plan.md中添加章节。</details></think>

---

明白了，你需要的是**技术蓝图**而不是操作手册。我来生成一份侧重于**步骤拆分和设计思路**的方案。

我建议创建一个新文件 `Implementation_Blueprint.md`（实施蓝图），内容包括：

## 📐 会包含的内容

### 1. 阶段划分
- 每个阶段的目标和产出物
- 阶段之间的依赖关系

### 2. 数据结构设计
- `data.json` 的完整Schema定义
- 差异计算算法的设计逻辑
- 新内容标记机制

### 3. 爬虫架构设计
- 模块划分（抓取/解析/对比/输出）
- 各平台数据源的处理策略
- 错误处理和容错机制

### 4. 关键决策点
- 数据更新策略（全量 vs 增量）
- 历史数据保留方案
- 多游戏扩展方式

### 5. 技术约束和注意事项
- GitHub Actions的限制
- 爬虫反爬策略
- 数据一致性保障

**是否创建这个新文件？** 还是你希望我直接在Dev_Plan.md中添加"详细实施蓝图"章节？

---

_**User (2025-12-17 11:08Z)**_

帮我直接整体更新这个文档吧

---

_**Agent (model claude-4.5-sonnet-thinking, mode Agent)**_

<think><details><summary>Thought Process</summary>
用户想让我直接整体更新Dev_Plan.md文档，加入详细的实施蓝图内容。我需要在原有内容的基础上，添加步骤拆分、设计思路、数据结构等内容。

根据用户规则，我需要先确认修改方案，然后再执行。让我先描述我要做的修改。</details></think>

---

好的，我会在现有Dev_Plan.md基础上，补充以下内容：

## 📝 将添加的章节

### 在原有6个章节后增加：

**7. 详细实施蓝图**
- 四个开发阶段的拆分和目标
- 每个阶段的产出物和验证点

**8. 数据结构设计**
- `data.json` 完整Schema定义
- 差异计算（diffs）的数据格式
- 新内容标记（is_new）机制

**9. 爬虫架构设计**
- 核心模块划分（数据获取/解析/对比/输出）
- 各平台适配策略
- 错误处理机制

**10. 关键技术决策**
- 全量更新 vs 增量更新的选择
- 历史数据管理策略
- 多游戏扩展方案

**11. 注意事项与限制**
- GitHub Actions使用限制
- 反爬虫应对策略
- 数据一致性保障

这样整个文档会从"方案概述"升级为"完整的技术蓝图"，既有宏观架构，又有实施细节。

**请确认是否按此方案更新文档？**

---

_**User (2025-12-17 11:08Z)**_

我的意思是整体更新，而不是追加更新

---

_**Agent (model claude-4.5-sonnet-thinking, mode Agent)**_

<think><details><summary>Thought Process</summary>
用户想要整体更新而不是追加更新，意思是重新组织整个文档的结构和内容，而不是在原有基础上添加新章节。我需要重新规划整个文档的架构，使其更加系统化和完整。

让我重新设计文档结构：
1. 项目概述
2. 架构设计
3. 数据结构设计
4. 技术实施方案（分阶段）
5. 核心模块设计（爬虫、自动化、前端）
6. 关键技术决策
7. 实施步骤
8. 注意事项

我应该先给出重构后的文档大纲，让用户确认。</details></think>

---

明白了，你想要**重新组织整个文档结构**，而不是在后面追加内容。

## 📋 重构后的文档结构

我会将现有内容重新整合为：

### 新的章节架构：

1. **项目概述** - 目标、特点、技术栈
2. **系统架构设计** - 四层架构（数据/后端/自动化/前端）+ 数据流向图
3. **数据结构设计** - data.json完整Schema、差异计算、标记机制
4. **核心模块设计**
   - 爬虫模块（架构、流程、平台适配）
   - 自动化模块（两个Workflow的作用和配置）
   - 前端模块（组件说明、数据读取）
5. **实施蓝图**（按阶段划分）
   - 阶段一：项目初始化
   - 阶段二：爬虫开发
   - 阶段三：自动化配置
   - 阶段四：部署上线
6. **关键技术决策** - 更新策略、扩展方案、容错机制
7. **项目目录结构** - 完整的文件树和说明
8. **注意事项与限制** - GitHub限制、反爬策略、最佳实践

### 重构思路：
- 从技术方案文档 → 完整技术蓝图
- 逻辑更清晰，先架构设计再实施细节
- 去掉冗余，增加设计思路说明

**确认按此结构重新编写整个文档？**

---

_**User (2025-12-17 11:08Z)**_

好的开始吧

---

_**Agent (model claude-4.5-sonnet-thinking, mode Agent)**_

<think><details><summary>Thought Process</summary>
用户确认了，我现在可以开始重写整个Dev_Plan.md文档。我需要按照刚才提出的结构，重新组织内容，使其成为一份完整的技术蓝图。

让我开始编写：
1. 项目概述
2. 系统架构设计
3. 数据结构设计
4. 核心模块设计
5. 实施蓝图
6. 关键技术决策
7. 项目目录结构
8. 注意事项与限制

我需要保留原有的有价值内容，但要重新组织结构，使其更加系统化和完整。</details></think>

---

好的，现在开始重写整个文档。

---

<tool-use data-tool-type="write" data-tool-name="write">

<details><summary>Tool use: **write** • Edit file: /Users/turbotang/Documents/工作/如临/竞品监控系统/echo_cms/Dev_Plan.md</summary>

**Chunk 1**
Lines added: 894, lines removed: 156

```diff
@@ -1,159 +1,897 @@
- # GitHub Serverless 竞品监控应用开发方案
+ # GitHub Serverless 竞品监控系统 - 技术蓝图
...
- 本方案旨在利用 GitHub 生态（Actions + Pages）实现一个零成本、自动化、可维护的竞品数据监控系统。
+ &gt; 基于 GitHub 生态（Actions + Pages）实现零成本、全自动、可维护的游戏竞品数据监控系统
...
- ## 1. 架构概览 (Architecture)
+ ---
...
- - **数据库 (Database)**: 不使用 MySQL/Postgres，直接使用 Git 仓库中的 &#96;public/data.json&#96; 文件作为轻量级 JSON 数据库。
- - **后端/爬虫 (Backend)**: Python 脚本，负责采集数据、与历史数据对比计算涨跌幅 (diffs)、生成最新 JSON。
- - **自动化 (CI/CD)**: GitHub Actions 定时任务（Cron Job），每日自动运行 Python 脚本并提交代码。
- - **前端 (Frontend)**: React (Vite)，部署在 GitHub Pages，通过 &#96;fetch('./data.json')&#96; 读取同源数据。
- 
- ## 2. 项目目录结构 (Directory Structure)
- 
- 建议在本地初始化项目时遵循以下结构：
- 
- &#96;&#96;&#96;
- my-game-monitor/
- ├── .github/
- │   └── workflows/
- │       ├── daily_crawl.yml   # 核心：每日定时抓取任务
- │       └── deploy_web.yml    # 核心：前端自动部署任务
- ├── crawler/
- │   ├── requirements.txt      # Python 依赖 (requests, beautifulsoup4, etc.)
- │   └── main.py               # 爬虫与数据处理主逻辑
- ├── public/
- │   └── data.json             # 【数据源】由 crawler 自动更新，前端读取
- ├── src/                      # React 源代码
- │   ├── App.jsx               # 主页面逻辑
- │   └── index.css             # Tailwind 样式
- ├── package.json
- └── vite.config.js
- &#96;&#96;&#96;
- 
- ## 3. 后端爬虫实现 (Python)
- 
- 在 &#96;crawler/main.py&#96; 中实现核心逻辑。你需要引入对比算法，确保前端能展示涨跌幅。
- 
- ### 关键逻辑伪代码：
- 
- &#96;&#96;&#96;python
- # 1. 读取旧数据
- old_data = load_json('../public/data.json')
- 
- # 2. 爬取新数据 (针对每个 Game ID)
- #    - 抓取基础数据 (预约/评分)
- #    - 抓取多平台动态 (TapTap/B站/微博)
- #    - 抓取评论
- 
- # 3. 计算差异 (Diff Calculation)
- #    对比 new_value 和 old_value，生成如 "+1.2k" 的字符串
- diffs = calculate_diff(new_data, old_data)
- 
- # 4. 标记新内容 (Highligting)
- #    如果 帖子ID 在 old_data 中不存在，标记 is_new = True
- 
- # 5. 写入文件
- save_json('../public/data.json', new_data_list)
- &#96;&#96;&#96;
- 
- ## 4. 自动化配置 (GitHub Actions)
- 
- ### A. 每日抓取任务 (&#96;.github/workflows/daily_crawl.yml&#96;)
- 
- &#96;&#96;&#96;yaml
- name: Daily Data Crawl
- 
- on:
-   schedule:
-     - cron: '0 0 * * *' # 每天 UTC 0点 (北京时间 8:00) 运行
-   workflow_dispatch:    # 允许手动触发
- 
- jobs:
-   crawl-and-update:
-     runs-on: ubuntu-latest
-     permissions:
-       contents: write   # 必须赋予写权限以提交 JSON
-     steps:
-       - name: Checkout code
-         uses: actions/checkout@v3
- 
-       - name: Set up Python
-         uses: actions/setup-python@v4
-         with:
-           python-version: '3.9'
- 
-       - name: Install dependencies
-         run: |
-           pip install requests beautifulsoup4
-           # 如果使用 playwright: pip install playwright &amp;&amp; playwright install chromium
- 
-       - name: Run Crawler
-         run: |
-           cd crawler
-           python main.py
- 
-       - name: Commit and Push
-         run: |
-           git config --global user.name 'GitHub Action'
-           git config --global user.email 'action@github.com'
-           git add public/data.json
-           # 如果有变更才提交，避免报错
-           git diff --quiet &amp;&amp; git diff --staged --quiet || (git commit -m "Auto-update data: $(date)" &amp;&amp; git push)
- &#96;&#96;&#96;
- 
- ### B. 前端部署任务 (&#96;.github/workflows/deploy_web.yml&#96;)
- 
- &#96;&#96;&#96;yaml
- name: Deploy Web
- 
- on:
-   push:
-     branches: [ main ] # 监听 main 分支 (包括爬虫提交的 data.json 更新)
- 
- jobs:
-   build-and-deploy:
-     runs-on: ubuntu-latest
-     steps:
-       - uses: actions/checkout@v3
-       - uses: actions/setup-node@v3
-         with:
-           node-version: 18
-       - run: npm install
-       - run: npm run build
-       
-       - name: Deploy to GitHub Pages
-         uses: peaceiris/actions-gh-pages@v3
-         with:
-           github_token: ${{ secrets.GITHUB_TOKEN }}
-           publish_dir: ./dist
- &#96;&#96;&#96;
- 
- ## 5. 快速上线步骤 (Checklist)
- 
- 1. **初始化仓库**: 在 GitHub 创建新仓库。
- 
- 2. **本地开发**:
-    - 创建 React 项目 (&#96;npm create vite@latest&#96;)
-    - 复制提供的 &#96;App.jsx&#96; 代码
-    - 编写 &#96;crawler/main.py&#96; 并本地测试生成 &#96;public/data.json&#96;
- 
- 3. **推送到 GitHub**: 
-    &#96;&#96;&#96;bash
-    git push origin main
-    &#96;&#96;&#96;
- 
- 4. **配置 Pages**: 
-    - 在 GitHub 仓库 Settings -&gt; Pages 中
-    - Source 选择 GitHub Actions
- 
- 5. **观察运行**:
-    - 查看 Actions 选项卡，确认 Deploy Web 成功运行
-    - 手动触发 Daily Data Crawl 任务测试爬虫是否正常运行并提交了数据
- 
- ## 6. 后续维护
- 
- - **添加新游戏**: 修改 &#96;crawler/main.py&#96; 中的游戏 ID 列表即可。
- - **修改爬虫规则**: 如果网站结构变化，仅需修改 Python 脚本，提交后下次调度自动生效。
- - **数据回滚**: 利用 Git 的历史记录功能，随时可以查看或回滚 &#96;public/data.json&#96; 到任意时间点。
- 
+ ## 1. 项目概述
+ 
+ ### 1.1 核心目标
+ 构建一个无需服务器和数据库的轻量级监控系统，实现对多款游戏在多个平台（TapTap、B站、微博、小红书）的数据自动采集、对比分析和可视化展示。
+ 
+ ### 1.2 技术特点
+ - **零成本运行**：完全基于 GitHub 免费服务
+ - **全自动化**：定时采集、自动计算、自动部署
+ - **版本可追溯**：利用 Git 记录所有历史数据变更
+ - **易于维护**：纯静态页面 + JSON 数据，无后端复杂度
+ 
+ ### 1.3 技术栈
+ - **前端**：React 18 + Vite + Tailwind CSS
+ - **爬虫**：Python 3.9 (requests/beautifulsoup4)
+ - **存储**：JSON 文件（Git 版本控制）
+ - **CI/CD**：GitHub Actions
+ - **部署**：GitHub Pages
+ 
+ ---
+ 
+ ## 2. 系统架构设计
+ 
+ ### 2.1 四层架构
+ 
+ &#96;&#96;&#96;
+ ┌─────────────────────────────────────────────────────────┐
+ │  展示层 (Presentation Layer)                             │
+ │  - React 单页应用                                         │
+ │  - 读取同源 data.json 展示看板                            │
+ │  - 部署在 GitHub Pages                                    │
+ └─────────────────────────────────────────────────────────┘
+                           ↑ fetch('./data.json')
+ ┌─────────────────────────────────────────────────────────┐
+ │  数据层 (Data Layer)                                      │
+ │  - public/data.json (轻量级 JSON 数据库)                  │
+ │  - Git 提供版本控制和历史回溯能力                          │
+ └─────────────────────────────────────────────────────────┘
+                           ↑ write &amp; commit
+ ┌─────────────────────────────────────────────────────────┐
+ │  处理层 (Processing Layer)                               │
+ │  - Python 爬虫脚本                                        │
+ │  - 数据采集 + 差异计算 + 新内容标记                        │
+ └─────────────────────────────────────────────────────────┘
+                           ↑ triggered by cron
+ ┌─────────────────────────────────────────────────────────┐
+ │  自动化层 (Automation Layer)                             │
+ │  - GitHub Actions Workflows                              │
+ │  - 定时任务 + 自动部署                                     │
+ └─────────────────────────────────────────────────────────┘
+ &#96;&#96;&#96;
+ 
+ ### 2.2 数据流向
+ 
+ &#96;&#96;&#96;
+ 每日 00:00 UTC
+     ↓
+ GitHub Actions 触发 daily_crawl.yml
+     ↓
+ 运行 crawler/main.py
+     ├─ 读取 public/data.json (旧数据)
+     ├─ 爬取各平台最新数据
+     ├─ 计算差异 (diffs)
+     └─ 标记新内容 (is_new)
+     ↓
+ 写入 public/data.json (新数据)
+     ↓
+ Git commit &amp; push
+     ↓
+ 触发 deploy_web.yml
+     ↓
+ 构建前端 (npm run build)
+     ↓
+ 部署到 GitHub Pages
+     ↓
+ 用户访问看板，自动获取最新数据
+ &#96;&#96;&#96;
+ 
+ ---
+ 
+ ## 3. 数据结构设计
+ 
+ ### 3.1 data.json 完整 Schema
+ 
+ &#96;&#96;&#96;json
+ [
+   {
+     "id": "game_unique_id",           // 游戏唯一标识
+     "title": "游戏名称",
+     "icon_char": "图",                // 图标字符（用于头像显示）
+     "last_updated": "2025-01-15 08:00", // 最后更新时间
+     
+     "basic_info": {
+       "rating": "9.5",                // 当前评分
+       "status": "预约中",             // 游戏状态
+       "tags": ["RPG", "二次元"],      // 标签数组
+       "reservations": "120万",         // 预约数
+       "followers": "45万",             // 关注数
+       "review_count": "8.5k",          // 评价数
+       
+       "diffs": {                      // 差异数据（核心）
+         "rating": {
+           "day": "+0.2",              // 日对比
+           "week": "+0.5",             // 周对比
+           "month": "+1.2"             // 月对比
+         },
+         "reservations": {
+           "day": "+5.2k",
+           "week": "+32k",
+           "month": "+80k"
+         },
+         "followers": { /*...*/ },
+         "review_count": { /*...*/ }
+       }
+     },
+     
+     "official_posts": {               // 官方动态（按平台分类）
+       "taptap": [
+         {
+           "date": "2025-01-14",
+           "title": "版本更新公告",
+           "comments": "1.2k",
+           "likes": "8.5k",
+           "is_new": true              // 新内容标记
+         }
+       ],
+       "bilibili": [ /*...*/ ],
+       "weibo": [ /*...*/ ],
+       "xiaohongshu": [ /*...*/ ]
+     },
+     
+     "hot_reviews": [                  // 热门评论
+       {
+         "user": "玩家昵称",
+         "score": 5,                   // 评分 1-5
+         "content": "评论内容...",
+         "replies": "123",
+         "likes": "456",
+         "is_new": false
+       }
+     ]
+   }
+ ]
+ &#96;&#96;&#96;
+ 
+ ### 3.2 差异计算逻辑
+ 
+ **核心算法思路：**
+ 
+ &#96;&#96;&#96;python
+ # 伪代码示例
+ def calculate_diff(new_value, old_value, period='day'):
+     """
+     计算数值差异并格式化为前端友好的字符串
+     
+     Args:
+         new_value: 新数据（字符串，如 "120万" 或 "9.5"）
+         old_value: 旧数据（对应周期的历史值）
+         period: 对比周期 (day/week/month)
+     
+     Returns:
+         格式化字符串，如 "+5.2k" / "-0.3" / "+80万"
+     """
+     # 1. 解析数值（处理 "万"、"k" 等单位）
+     new_num = parse_number(new_value)
+     old_num = parse_number(old_value)
+     
+     # 2. 计算差值
+     diff = new_num - old_num
+     
+     # 3. 格式化输出（保留符号和单位）
+     if diff &gt; 0:
+         return f"+{format_number(diff)}"
+     elif diff &lt; 0:
+         return f"{format_number(diff)}"  # 负号自带
+     else:
+         return "0"
+ &#96;&#96;&#96;
+ 
+ **关键点：**
+ - 需要统一数值单位（万、k、原始数字）
+ - 保留正负号（+/-）用于前端颜色判断
+ - 支持三个时间维度的对比（日/周/月）
+ 
+ ### 3.3 新内容标记机制
+ 
+ **标记规则：**
+ - 官方动态：帖子ID在旧数据中不存在 → &#96;is_new: true&#96;
+ - 热门评论：评论ID在旧数据中不存在 → &#96;is_new: true&#96;
+ - 有效期：标记保留7天，过期后自动移除
+ 
+ **前端展示效果：**
+ - &#96;is_new: true&#96; → 蓝色高亮边框 + "NEW" 标签 + 动画效果
+ - &#96;is_new: false&#96; → 常规灰色样式
+ 
+ ---
+ 
+ ## 4. 核心模块设计
+ 
+ ### 4.1 爬虫模块 (crawler/main.py)
+ 
+ #### 模块划分
+ 
+ &#96;&#96;&#96;
+ crawler/
+ ├── main.py              # 入口脚本
+ ├── fetchers/            # 数据获取器（按平台分）
+ │   ├── taptap.py
+ │   ├── bilibili.py
+ │   ├── weibo.py
+ │   └── xiaohongshu.py
+ ├── parser.py            # 数据解析与清洗
+ ├── comparator.py        # 差异计算引擎
+ └── utils.py             # 工具函数（数字解析、格式化等）
+ &#96;&#96;&#96;
+ 
+ #### 执行流程
+ 
+ &#96;&#96;&#96;
+ 1. 加载配置
+    ├─ 读取 games_config.json (游戏ID列表)
+    └─ 读取 public/data.json (历史数据)
+ 
+ 2. 数据采集（并发执行）
+    ├─ TapTap: 预约数、评分、评论
+    ├─ B站: 官方动态、视频数据
+    ├─ 微博: 官方微博内容
+    └─ 小红书: 笔记数据
+ 
+ 3. 数据处理
+    ├─ 解析与清洗（统一格式）
+    ├─ 差异计算（与历史对比）
+    └─ 新内容标记（ID去重）
+ 
+ 4. 数据输出
+    ├─ 生成新的 data.json
+    ├─ 备份旧数据（可选）
+    └─ 记录日志
+ 
+ 5. 错误处理
+    ├─ 单个平台失败不影响其他平台
+    ├─ 保留上一次成功的数据
+    └─ 记录错误日志用于排查
+ &#96;&#96;&#96;
+ 
+ #### 平台适配策略
+ 
+ | 平台 | 数据源 | 抓取方式 | 反爬应对 |
+ |-----|--------|---------|---------|
+ | TapTap | Web + API | requests | User-Agent轮换 + 延迟请求 |
+ | B站 | 视频页/动态API | requests | Cookie池 + IP代理（可选）|
+ | 微博 | 移动端API | requests | 登录态 + 限频控制 |
+ | 小红书 | Web接口 | selenium/playwright | 模拟浏览器 + 滑块验证 |
+ 
+ #### 容错机制
+ 
+ &#96;&#96;&#96;python
+ # 伪代码示例
+ def fetch_with_retry(url, max_retries=3):
+     for i in range(max_retries):
+         try:
+             response = requests.get(url, timeout=10)
+             return response.json()
+         except Exception as e:
+             if i == max_retries - 1:
+                 log_error(f"Failed after {max_retries} attempts: {e}")
+                 return None  # 返回空，使用旧数据
+             time.sleep(2 ** i)  # 指数退避
+ &#96;&#96;&#96;
+ 
+ ### 4.2 自动化模块 (GitHub Actions)
+ 
+ #### Workflow 1: 每日数据抓取
+ 
+ **文件路径**: &#96;.github/workflows/daily_crawl.yml&#96;
+ 
+ **核心配置要点**:
+ &#96;&#96;&#96;yaml
+ # 触发条件
+ on:
+   schedule:
+     - cron: '0 0 * * *'  # 每天UTC 0点 (北京8点)
+   workflow_dispatch:      # 支持手动触发（重要！）
+ 
+ # 权限配置（必须）
+ permissions:
+   contents: write         # 允许提交代码
+ 
+ # 关键步骤
+ steps:
+   - 检出代码
+   - 配置Python环境
+   - 安装依赖
+   - 运行爬虫
+   - 提交变更（仅当有更新时）
+ &#96;&#96;&#96;
+ 
+ **注意事项**:
+ - 必须赋予 &#96;contents: write&#96; 权限
+ - 提交前检查是否有变更（避免空提交）
+ - 使用 &#96;workflow_dispatch&#96; 方便调试
+ 
+ #### Workflow 2: 前端自动部署
+ 
+ **文件路径**: &#96;.github/workflows/deploy_web.yml&#96;
+ 
+ **触发机制**:
+ &#96;&#96;&#96;yaml
+ on:
+   push:
+     branches: [ main ]  # 监听main分支的push（包括爬虫提交）
+ &#96;&#96;&#96;
+ 
+ **部署流程**:
+ &#96;&#96;&#96;
+ 1. 检出代码（包含最新的 data.json）
+ 2. 安装 Node.js 依赖
+ 3. 构建前端 (npm run build)
+ 4. 部署 dist/ 到 gh-pages 分支
+ &#96;&#96;&#96;
+ 
+ **关键点**:
+ - 使用 &#96;peaceiris/actions-gh-pages@v3&#96; 插件
+ - &#96;publish_dir&#96; 设置为 &#96;./dist&#96;
+ - &#96;github_token&#96; 自动注入，无需手动配置
+ 
+ ### 4.3 前端模块 (React App)
+ 
+ #### 组件架构
+ 
+ &#96;&#96;&#96;
+ App.jsx (主容器)
+ ├── Header (顶部栏)
+ │   ├── Logo
+ │   ├── 更新时间显示
+ │   └── 调试导入按钮
+ │
+ ├── GameColumn (游戏列)【核心组件】
+ │   ├── ProductHeader (产品信息)
+ │   │   ├── 图标 + 标题
+ │   │   ├── 状态标签 + 评分
+ │   │   └── 标签列表
+ │   │
+ │   ├── SectionCard: 基础数据
+ │   │   ├── 周期切换器 (日/周/月)
+ │   │   └── 4个 MiniStat (预约/关注/评价/评分)
+ │   │
+ │   ├── SectionCard: 官方动态
+ │   │   ├── 平台Tab (Tap/B站/微博/红书)
+ │   │   └── CompactTimelineItem 列表
+ │   │
+ │   └── SectionCard: 热门舆情
+ │       └── CompactReviewItem 列表
+ │
+ └── ImportModal (调试模态框)
+ &#96;&#96;&#96;
+ 
+ #### 数据读取逻辑
+ 
+ &#96;&#96;&#96;javascript
+ // 生产环境：从同源读取
+ useEffect(() =&gt; {
+   fetch('./data.json')
+     .then(res =&gt; res.json())
+     .then(data =&gt; setGames(data))
+     .catch(err =&gt; {
+       // 失败时使用 Fallback 数据
+       console.log("使用默认数据");
+     });
+ }, []);
+ &#96;&#96;&#96;
+ 
+ #### 状态管理
+ 
+ &#96;&#96;&#96;javascript
+ // 全局状态
+ const [games, setGames] = useState([]);           // 游戏数据列表
+ const [comparisonPeriod, setComparisonPeriod] = useState('day'); // 对比周期
+ const [globalPlatform, setGlobalPlatform] = useState('taptap');  // 当前平台
+ 
+ // 涨跌幅自动取值
+ game.basic_info.diffs?.reservations?.[comparisonPeriod]
+ &#96;&#96;&#96;
+ 
+ ---
+ 
+ ## 5. 实施蓝图
+ 
+ ### 阶段一：项目初始化
+ 
+ **目标**: 搭建前端框架，验证UI可正常运行
+ 
+ **产出物**:
+ - Vite + React 项目结构
+ - Tailwind CSS 配置完成
+ - App.jsx 集成并能渲染 Fallback 数据
+ 
+ **验证点**:
+ - &#96;npm run dev&#96; 启动成功
+ - 浏览器能看到示例游戏卡片
+ - 调试模态框能导入JSON并更新看板
+ 
+ **关键文件**:
+ &#96;&#96;&#96;
+ ├── package.json         # 依赖：react, tailwindcss, lucide-react
+ ├── vite.config.js       # base: '/repo-name/' (重要！)
+ ├── index.html
+ ├── src/
+ │   ├── main.jsx
+ │   ├── App.jsx          # 已提供
+ │   └── index.css        # Tailwind 配置
+ └── public/
+     └── data.json        # 初始测试数据（手动创建）
+ &#96;&#96;&#96;
+ 
+ ### 阶段二：爬虫开发
+ 
+ **目标**: 实现数据采集和差异计算核心逻辑
+ 
+ **产出物**:
+ - &#96;crawler/main.py&#96; 可本地运行
+ - 能生成符合 Schema 的 &#96;data.json&#96;
+ - 差异计算算法验证通过
+ 
+ **验证点**:
+ - 手动运行 &#96;python main.py&#96; 无报错
+ - 生成的 &#96;data.json&#96; 能被前端正确读取
+ - 涨跌幅数据正确显示（红涨绿跌）
+ - 新内容标记正常生效（蓝色高亮）
+ 
+ **开发建议**:
+ 1. 先实现单个平台的数据抓取
+ 2. 然后实现差异计算算法
+ 3. 最后扩展到多平台、多游戏
+ 4. 使用测试数据验证边界情况
+ 
+ **数据对比策略**:
+ &#96;&#96;&#96;
+ 日对比：当前值 vs 昨天同时刻的值
+ 周对比：当前值 vs 7天前的值
+ 月对比：当前值 vs 30天前的值
+ 
+ 实现方式：
+ - 方案1：读取 Git 历史中的 data.json (推荐)
+ - 方案2：在 data.json 中保留历史快照字段
+ &#96;&#96;&#96;
+ 
+ ### 阶段三：自动化配置
+ 
+ **目标**: 实现定时自动抓取和自动部署
+ 
+ **产出物**:
+ - &#96;.github/workflows/daily_crawl.yml&#96;
+ - &#96;.github/workflows/deploy_web.yml&#96;
+ - 两个 Workflow 能正常触发并执行
+ 
+ **验证点**:
+ - 手动触发 &#96;daily_crawl&#96; 能成功运行
+ - 爬虫运行后自动提交 data.json
+ - 提交后自动触发 &#96;deploy_web&#96;
+ - GitHub Pages 能访问并看到最新数据
+ 
+ **配置清单**:
+ &#96;&#96;&#96;yaml
+ # daily_crawl.yml 关键配置
+ permissions:
+   contents: write  # 必须
+ 
+ env:
+   TZ: 'Asia/Shanghai'  # 可选，用于日志时间
+ 
+ # 提交逻辑（仅当有变更时提交）
+ git diff --quiet &amp;&amp; git diff --staged --quiet || \
+   (git commit -m "Auto-update data" &amp;&amp; git push)
+ &#96;&#96;&#96;
+ 
+ &#96;&#96;&#96;yaml
+ # deploy_web.yml 关键配置
+ - name: Deploy to GitHub Pages
+   uses: peaceiris/actions-gh-pages@v3
+   with:
+     github_token: ${{ secrets.GITHUB_TOKEN }}
+     publish_dir: ./dist
+ &#96;&#96;&#96;
+ 
+ ### 阶段四：部署上线
+ 
+ **目标**: 完整系统上线，进入自动运行状态
+ 
+ **产出物**:
+ - 代码推送到 GitHub 仓库
+ - GitHub Pages 配置完成
+ - 可访问的线上地址
+ 
+ **上线步骤**:
+ 1. 在 GitHub 创建新仓库（public）
+ 2. 本地项目关联远程仓库
+ 3. 推送所有代码到 main 分支
+ 4. 仓库 Settings → Pages → Source 选择 "GitHub Actions"
+ 5. 等待首次部署完成，获取访问地址
+ 6. 手动触发一次 &#96;daily_crawl&#96; 验证完整流程
+ 
+ **验证点**:
+ - 线上页面能正常访问
+ - 数据正确显示（非 Fallback 数据）
+ - 第二天自动更新生效
+ - 涨跌幅数据正确计算
+ 
+ ---
+ 
+ ## 6. 关键技术决策
+ 
+ ### 6.1 数据更新策略
+ 
+ **选择：全量更新（推荐）**
+ 
+ **理由**:
+ - 数据量小（JSON文件 &lt; 1MB）
+ - 实现简单，无需维护增量逻辑
+ - Git 自动处理差异压缩
+ 
+ **流程**:
+ &#96;&#96;&#96;
+ 每次运行爬虫：
+ 1. 读取旧的 data.json
+ 2. 爬取所有游戏的全量数据
+ 3. 计算差异后生成新的 data.json
+ 4. 完全覆盖旧文件
+ &#96;&#96;&#96;
+ 
+ **优化方向**:
+ - 如果数据量增大（&gt;10MB），考虑拆分文件（按游戏）
+ - 使用 gzip 压缩（前端 fetch 自动解压）
+ 
+ ### 6.2 历史数据管理
+ 
+ **方案：利用 Git History（推荐）**
+ 
+ **实现**:
+ &#96;&#96;&#96;python
+ # 获取N天前的数据
+ import subprocess
+ import json
+ 
+ def get_history_data(days_ago):
+     """通过 Git 获取历史数据"""
+     cmd = f"git show HEAD~{days_ago}:public/data.json"
+     result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
+     return json.loads(result.stdout)
+ 
+ # 使用示例
+ old_data_7days = get_history_data(7)  # 7天前的数据
+ &#96;&#96;&#96;
+ 
+ **优点**:
+ - 无需额外存储空间
+ - 可追溯任意时间点的数据
+ - 天然支持回滚
+ 
+ **注意事项**:
+ - GitHub Actions 默认只检出最近一次提交
+ - 需要在 Workflow 中配置 &#96;fetch-depth: 30&#96; 获取历史
+ 
+ &#96;&#96;&#96;yaml
+ - uses: actions/checkout@v3
+   with:
+     fetch-depth: 30  # 获取最近30天的提交历史
+ &#96;&#96;&#96;
+ 
+ ### 6.3 多游戏扩展方案
+ 
+ **配置文件方式（推荐）**
+ 
+ **创建 &#96;games_config.json&#96;**:
+ &#96;&#96;&#96;json
+ [
+   {
+     "id": "game1",
+     "name": "游戏A",
+     "taptap_id": "12345",
+     "bilibili_uid": "67890",
+     "weibo_uid": "abcdef",
+     "xiaohongshu_uid": "xyz123"
+   },
+   {
+     "id": "game2",
+     "name": "游戏B",
+     // ...
+   }
+ ]
+ &#96;&#96;&#96;
+ 
+ **爬虫读取配置**:
+ &#96;&#96;&#96;python
+ with open('games_config.json') as f:
+     games = json.load(f)
+ 
+ for game in games:
+     data = fetch_game_data(game['taptap_id'], game['bilibili_uid'])
+     # ...
+ &#96;&#96;&#96;
+ 
+ **扩展新游戏**:
+ - 只需在配置文件中添加一条记录
+ - 无需修改爬虫代码
+ 
+ ### 6.4 容错与降级
+ 
+ **分级容错策略**:
+ 
+ &#96;&#96;&#96;
+ Level 1: 单个平台失败
+   └─ 保留该平台的旧数据，其他平台正常更新
+ 
+ Level 2: 单个游戏失败
+   └─ 保留该游戏的旧数据，其他游戏正常更新
+ 
+ Level 3: 爬虫整体失败
+   └─ 不提交代码，等待下次执行
+ 
+ Level 4: 无法恢复的错误
+   └─ 发送通知（通过 GitHub Issues 或邮件）
+ &#96;&#96;&#96;
+ 
+ **实现示例**:
+ &#96;&#96;&#96;python
+ def safe_fetch(fetch_func, fallback_data):
+     try:
+         return fetch_func()
+     except Exception as e:
+         log_error(e)
+         return fallback_data  # 使用旧数据
+ &#96;&#96;&#96;
+ 
+ ---
+ 
+ ## 7. 项目目录结构
+ 
+ &#96;&#96;&#96;
+ my-game-monitor/
+ ├── .github/
+ │   └── workflows/
+ │       ├── daily_crawl.yml         # 定时抓取任务（每日 0:00 UTC）
+ │       └── deploy_web.yml          # 前端自动部署任务
+ │
+ ├── crawler/                        # Python 爬虫模块
+ │   ├── main.py                     # 入口脚本
+ │   ├── fetchers/                   # 平台适配器
+ │   │   ├── __init__.py
+ │   │   ├── taptap.py               # TapTap 数据抓取
+ │   │   ├── bilibili.py             # B站数据抓取
+ │   │   ├── weibo.py                # 微博数据抓取
+ │   │   └── xiaohongshu.py          # 小红书数据抓取
+ │   ├── parser.py                   # 数据解析与清洗
+ │   ├── comparator.py               # 差异计算引擎
+ │   ├── utils.py                    # 工具函数
+ │   ├── requirements.txt            # Python 依赖
+ │   └── games_config.json           # 游戏配置（游戏ID、平台账号等）
+ │
+ ├── public/
+ │   └── data.json                   # 【核心数据文件】爬虫输出 + 前端读取
+ │
+ ├── src/                            # React 前端源码
+ │   ├── main.jsx                    # 入口文件
+ │   ├── App.jsx                     # 主组件（已提供）
+ │   └── index.css                   # Tailwind 样式配置
+ │
+ ├── .gitignore                      # 忽略 node_modules, __pycache__ 等
+ ├── index.html                      # HTML 模板
+ ├── package.json                    # Node.js 依赖配置
+ ├── vite.config.js                  # Vite 构建配置
+ ├── tailwind.config.js              # Tailwind 配置
+ └── README.md                       # 项目说明文档
+ &#96;&#96;&#96;
+ 
+ **关键文件说明**:
+ 
+ | 文件 | 作用 | 维护方式 |
+ |-----|------|---------|
+ | &#96;data.json&#96; | 核心数据文件 | 爬虫自动更新，Git 自动追踪 |
+ | &#96;games_config.json&#96; | 游戏配置 | 手动维护（添加新游戏时修改）|
+ | &#96;daily_crawl.yml&#96; | 定时任务配置 | 一次配置，无需改动 |
+ | &#96;App.jsx&#96; | 前端主组件 | 需要调整UI时修改 |
+ | &#96;vite.config.js&#96; | 构建配置 | **必须配置 base 路径** |
+ 
+ **vite.config.js 示例**:
+ &#96;&#96;&#96;javascript
+ import { defineConfig } from 'vite'
+ import react from '@vitejs/plugin-react'
+ 
+ export default defineConfig({
+   plugins: [react()],
+   base: '/your-repo-name/',  // 重要！必须与仓库名一致
+ })
+ &#96;&#96;&#96;
+ 
+ ---
+ 
+ ## 8. 注意事项与限制
+ 
+ ### 8.1 GitHub Actions 使用限制
+ 
+ | 限制项 | 公开仓库 | 私有仓库 |
+ |--------|---------|---------|
+ | 月执行时间 | 无限制 | 2000分钟 |
+ | 单任务最长时间 | 6小时 | 6小时 |
+ | 并发任务数 | 20个 | 5个 |
+ | 存储空间 | 500MB | 500MB |
+ 
+ **建议**:
+ - 使用公开仓库（无限制）
+ - 单次爬虫运行控制在 5 分钟内
+ - 避免在 Workflow 中存储大文件
+ 
+ ### 8.2 反爬虫应对策略
+ 
+ **基础防护**:
+ &#96;&#96;&#96;python
+ headers = {
+     'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) ...',
+     'Referer': 'https://www.taptap.cn/',
+     'Accept-Language': 'zh-CN,zh;q=0.9',
+ }
+ 
+ # 随机延迟
+ import random
+ time.sleep(random.uniform(1, 3))
+ &#96;&#96;&#96;
+ 
+ **进阶方案**:
+ - **IP代理**: 使用免费代理池或付费服务（如有必要）
+ - **Cookie池**: 维护多个账号的登录态
+ - **请求限频**: 控制每分钟请求数（如 10 req/min）
+ - **模拟浏览器**: 使用 Playwright（针对强反爬平台）
+ 
+ **风险控制**:
+ - 不要频繁请求同一页面
+ - 尊重 &#96;robots.txt&#96; 规则
+ - 出现验证码时降级到手动更新
+ 
+ ### 8.3 数据一致性保障
+ 
+ **场景1：爬虫运行中断**
+ - 使用事务式写入（写临时文件 → 验证 → 重命名）
+ - 确保 data.json 始终是完整的
+ 
+ &#96;&#96;&#96;python
+ import json
+ import os
+ 
+ # 安全写入
+ with open('data.json.tmp', 'w') as f:
+     json.dump(data, f)
+ 
+ # 验证 JSON 有效性
+ with open('data.json.tmp') as f:
+     json.load(f)  # 如果格式错误会抛异常
+ 
+ # 原子操作替换
+ os.replace('data.json.tmp', 'data.json')
+ &#96;&#96;&#96;
+ 
+ **场景2：部分平台数据缺失**
+ - 保留旧数据，标记更新失败
+ - 在前端显示 "数据更新中" 提示
+ 
+ &#96;&#96;&#96;json
+ {
+   "official_posts": {
+     "taptap": [...],
+     "bilibili": null,  // 标记为 null 表示抓取失败
+     "_meta": {
+       "failed_platforms": ["bilibili"],
+       "last_error": "timeout"
+     }
+   }
+ }
+ &#96;&#96;&#96;
+ 
+ ### 8.4 性能优化建议
+ 
+ **前端优化**:
+ - 启用 &#96;gzip&#96; 压缩（GitHub Pages 默认支持）
+ - 图片使用 CDN（如有）
+ - 使用 React.memo 优化渲染
+ 
+ **爬虫优化**:
+ - 使用异步请求（&#96;asyncio&#96; + &#96;aiohttp&#96;）
+ - 并发抓取多个游戏
+ - 缓存不常变的数据（如游戏基础信息）
+ 
+ **数据优化**:
+ - 移除不必要的字段
+ - 数值转为数字类型（减少字符串占用）
+ - 压缩 JSON（去除空格）
+ 
+ &#96;&#96;&#96;python
+ # 压缩 JSON
+ json.dump(data, f, separators=(',', ':'))  # 无空格
+ &#96;&#96;&#96;
+ 
+ ### 8.5 最佳实践
+ 
+ 1. **开发阶段**
+    - 先用 Fallback 数据验证前端
+    - 再用真实数据验证爬虫
+    - 最后联调自动化流程
+ 
+ 2. **调试技巧**
+    - 使用前端的"导入/调试"功能快速验证数据格式
+    - GitHub Actions 支持手动触发（&#96;workflow_dispatch&#96;）
+    - 查看 Actions 日志排查错误
+ 
+ 3. **维护建议**
+    - 定期检查 Actions 运行状态（每周一次）
+    - 遇到持续失败时及时修复
+    - 添加新游戏后手动触发一次验证
+ 
+ 4. **扩展方向**
+    - 增加数据可视化（图表展示趋势）
+    - 支持数据导出（CSV/Excel）
+    - 添加告警功能（数据异常时通知）
+    - 接入更多平台（Discord、Reddit 等）
+ 
+ ---
+ 
+ ## 附录：快速启动清单
+ 
+ ### 本地开发环境准备
+ &#96;&#96;&#96;bash
+ # 前端依赖
+ node &gt;= 18
+ npm &gt;= 9
+ 
+ # 后端依赖
+ python &gt;= 3.9
+ pip install requests beautifulsoup4
+ &#96;&#96;&#96;
+ 
+ ### 从零开始的完整流程
+ 
+ **Step 1**: 在 GitHub 创建新仓库（public）
+ 
+ **Step 2**: 本地初始化项目
+ &#96;&#96;&#96;bash
+ npm create vite@latest my-game-monitor -- --template react
+ cd my-game-monitor
+ npm install tailwindcss lucide-react
+ &#96;&#96;&#96;
+ 
+ **Step 3**: 复制核心文件
+ - 将提供的 &#96;App.jsx&#96; 复制到 &#96;src/&#96;
+ - 配置 Tailwind CSS
+ - 创建初始 &#96;data.json&#96;
+ 
+ **Step 4**: 编写爬虫脚本
+ - 创建 &#96;crawler/main.py&#96;
+ - 本地测试生成数据
+ 
+ **Step 5**: 配置 GitHub Actions
+ - 创建 &#96;.github/workflows/&#96; 目录
+ - 添加两个 YAML 配置文件
+ 
+ **Step 6**: 推送并部署
+ &#96;&#96;&#96;bash
+ git init
+ git add .
+ git commit -m "Initial commit"
+ git remote add origin &lt;your-repo-url&gt;
+ git push -u origin main
+ &#96;&#96;&#96;
+ 
+ **Step 7**: 配置 GitHub Pages
+ - 仓库 Settings → Pages → Source: GitHub Actions
+ 
+ **Step 8**: 验证
+ - 访问 &#96;https://&lt;username&gt;.github.io/&lt;repo-name&gt;/&#96;
+ - 手动触发爬虫任务测试
+ 
+ ---
+ 
+ ## 总结
+ 
+ 本方案通过巧妙利用 GitHub 的免费资源，实现了一个完全自动化、零运维成本的竞品监控系统。核心优势在于：
+ 
+ - **简单**: 无需服务器、数据库，仅需配置一次
+ - **可靠**: 利用 Git 版本控制，数据永不丢失
+ - **灵活**: 易于扩展新游戏、新平台
+ - **免费**: 完全基于 GitHub 免费服务
+ 
+ 适用于个人开发者、小团队进行竞品数据监控，后续可根据需要升级到付费服务。
+ 
```

</details>

</tool-use>

---

<think><details><summary>Thought Process</summary>
好的，我已经完成了整个文档的重写。这是一份完整的技术蓝图，包含了：

1. 项目概述 - 目标、特点、技术栈
2. 系统架构设计 - 四层架构图和数据流向
3. 数据结构设计 - 完整Schema、差异计算逻辑、标记机制
4. 核心模块设计 - 爬虫、自动化、前端三大模块的详细设计
5. 实施蓝图 - 四个阶段的目标、产出物、验证点
6. 关键技术决策 - 更新策略、历史管理、扩展方案、容错机制
7. 项目目录结构 - 完整的文件树和说明
8. 注意事项与限制 - GitHub限制、反爬策略、性能优化
9. 附录 - 快速启动清单

这份文档现在是一个完整的技术蓝图，既有宏观架构设计，也有实施细节思路。用户可以按照这个蓝图来开发整个系统。</details></think>

---

✅ **文档重构完成！**

我已经将Dev_Plan.md完全重写为一份**完整的技术蓝图**，主要改进：

## 📊 新结构对比

**原文档**：基础方案文档（6章节）  
**新文档**：完整技术蓝图（8章节 + 附录）

## ✨ 核心亮点

### 1. **系统架构可视化**
- 四层架构图（展示/数据/处理/自动化）
- 完整数据流向图
- 模块之间的关系清晰

### 2. **数据结构完整定义**
- `data.json` 完整Schema
- 差异计算算法设计逻辑（伪代码）
- 新内容标记机制说明

### 3. **核心模块深度设计**
- **爬虫模块**：目录划分、执行流程、平台适配、容错机制
- **自动化模块**：两个Workflow的配置要点和注意事项
- **前端模块**：组件架构、数据读取、状态管理

### 4. **实施蓝图（4阶段）**
每个阶段都明确了：
- 目标是什么
- 产出物是什么
- 如何验证完成

### 5. **技术决策说明**
- 为什么选择全量更新
- 历史数据如何管理（Git History方案）
- 多游戏扩展方案（配置文件方式）
- 分级容错策略

### 6. **实用参考资料**
- GitHub Actions限制清单
- 反爬虫策略表格
- 性能优化建议
- 快速启动清单

## 📖 文档使用建议

- **第1-2章**：理解整体架构
- **第3章**：确定数据格式
- **第4章**：开发各模块时查阅
- **第5章**：按阶段执行开发
- **第6-8章**：遇到问题时查阅

现在你可以基于这份蓝图开始实施开发了！需要我协助哪个具体模块的开发吗？

---

